{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "获取大模型",
   "id": "85afb3a36d53346c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T06:55:37.132795Z",
     "start_time": "2025-12-09T06:55:25.686047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#导入 dotenv 库的 load_dotenv 函数，用于加载环境变量文件（.env）中的配置\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os\n",
    "\n",
    "dotenv.load_dotenv()  \n",
    "#加载当前目录下的 .env 文件\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv(\"DASHSCOPE_API_KEY\")\n",
    "os.environ['OPENAI_BASE_URL'] = os.getenv(\"DASHSCOPE_BASE_URL\")\n",
    "\n",
    "#创建大模型实例\n",
    "llm = ChatOpenAI(model=\"qwen-turbo\") \n",
    "# 直接提供问题，并调用llm\n",
    "response = llm.invoke(\"什么是大模型？\")\n",
    "print(response)"
   ],
   "id": "a4da4d2a15a4822b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='“大模型”通常指的是**大规模机器学习模型**，特别是在人工智能（AI）领域，尤其是**深度学习**中，指参数量非常庞大、训练数据量也非常大的模型。这些模型能够捕捉更复杂的模式，具有更强的表达能力和泛化能力。\\n\\n---\\n\\n### 一、什么是“大模型”？\\n\\n**大模型**（Large Model）一般是指：\\n\\n- **参数量巨大**：比如超过10亿（1B）、100亿（10B）、甚至数千亿（1T）个参数。\\n- **训练数据量庞大**：使用海量文本、图像或其他数据进行训练。\\n- **具备强大的通用性**：可以在多种任务上表现良好，而不仅仅是特定任务。\\n\\n---\\n\\n### 二、大模型的主要特点\\n\\n| 特点 | 说明 |\\n|------|------|\\n| 参数量大 | 比传统模型多出几个数量级，如GPT-3有1750亿参数 |\\n| 训练数据多 | 使用数TB到PB级别的数据进行训练 |\\n| 表达能力强 | 能理解和生成自然语言、代码、图像等复杂内容 |\\n| 通用性强 | 不仅能做特定任务（如分类），还能处理多种任务（如问答、翻译、推理） |\\n| 需要强大算力 | 训练和推理都需要高性能计算资源（如GPU/TPU集群） |\\n\\n---\\n\\n### 三、常见类型的大模型\\n\\n#### 1. **语言模型（Language Models）**\\n- 如：GPT系列（GPT-3, GPT-4）、BERT、T5、Qwen（通义千问）、ChatGLM、LLaMA 等\\n- 功能：文本生成、问答、对话、翻译、代码生成等\\n\\n#### 2. **视觉模型（Vision Models）**\\n- 如：CLIP、ViT（Vision Transformer）\\n- 功能：图像识别、图像生成、图文理解等\\n\\n#### 3. **多模态模型（Multimodal Models）**\\n- 如：GPT-4（支持图文输入）、CLIP、Flamingo\\n- 功能：同时处理文本、图像、音频等多种信息\\n\\n#### 4. **代码生成模型**\\n- 如：Codex、GitHub Copilot、CodeLlama\\n- 功能：根据自然语言生成代码、代码补全、代码解释等\\n\\n---\\n\\n### 四、大模型的应用场景\\n\\n- **自然语言处理（NLP）**：聊天机器人、智能客服、内容生成\\n- **代码生成与理解**：编程助手、自动化代码生成\\n- **内容创作**：文章撰写、剧本创作、诗歌生成\\n- **数据分析与推理**：从大量数据中提取信息、逻辑推理\\n- **多模态任务**：图文生成、视频理解、语音合成等\\n\\n---\\n\\n### 五、大模型的挑战\\n\\n- **训练成本高**：需要大量算力和电力\\n- **推理成本高**：部署和运行大模型需要高性能硬件\\n- **模型可解释性差**：黑盒特性使得模型决策难以理解\\n- **数据隐私和安全问题**：训练数据可能包含敏感信息\\n- **伦理与偏见问题**：模型可能继承训练数据中的偏见或错误信息\\n\\n---\\n\\n### 六、大模型的发展趋势\\n\\n- **模型越来越小但更高效**：如蒸馏技术、量化、剪枝等让模型更轻量\\n- **专用模型越来越多**：针对特定任务（如医疗、金融）优化的模型\\n- **大模型 + 小模型结合**：用大模型做核心，小模型做边缘处理\\n- **大模型开源化**：如Meta的LLaMA、阿里云的Qwen等开放模型\\n- **可控性和安全性提升**：如通过提示工程、微调等方式控制输出\\n\\n---\\n\\n### 七、总结\\n\\n> 大模型是当前人工智能发展的重要方向，它代表了**更强的通用性、更高的性能和更广泛的应用潜力**。虽然面临诸多挑战，但它正在深刻地改变我们与AI互动的方式。\\n\\n如果你对某个具体的大模型（如GPT、BERT、Qwen）感兴趣，我可以进一步介绍它的原理和应用。' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 894, 'prompt_tokens': 16, 'total_tokens': 910, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'qwen-turbo', 'system_fingerprint': None, 'id': 'chatcmpl-337f29c8-52dc-4fc2-99dd-4c6c7c0ba6e0', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--02789842-5fe4-4b94-815e-ae384d56a07c-0' usage_metadata={'input_tokens': 16, 'output_tokens': 894, 'total_tokens': 910, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "2、使用提示词模板",
   "id": "816b10c0000679ca"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T06:56:23.296200Z",
     "start_time": "2025-12-09T06:56:10.230824Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "# 需要注意的一点是，这里需要指明具体的role，在这里是system和用户\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是世界级的技术文档编写者\"),\n",
    "    (\"user\", \"{input}\")  # {input}为变量\n",
    "])\n",
    "\n",
    "# 我们可以把prompt和具体llm的调用和在一起。\n",
    "chain = prompt | llm\n",
    "message = chain.invoke({\"input\": \"大模型中的LangChain是什么?\"})\n",
    "print(message)\n",
    "\n",
    "# print(type(message))"
   ],
   "id": "3bfcd5d4040357d3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='LangChain 是一个用于构建 **基于大语言模型（LLM）** 应用程序的框架，它提供了一套工具、库和接口，使开发者能够更高效地设计、训练和部署基于大模型的系统。LangChain 不仅仅是一个简单的 API 调用封装，而是一个全面的开发平台，支持从数据处理、模型集成到应用构建的全流程。\\n\\n---\\n\\n## 一、LangChain 的核心概念\\n\\n### 1. **Chain（链）**\\n- Chain 是 LangChain 中的核心抽象，表示一系列操作的组合。\\n- 每个 Chain 可以是：\\n  - 一个单独的 LLM 操作\\n  - 一个提示模板 + LLM 的组合\\n  - 多个 Chain 的组合（例如：将用户输入通过多个步骤处理）\\n\\n#### 示例：\\n```python\\nfrom langchain.chains import LLMChain\\nfrom langchain.prompts import PromptTemplate\\nfrom langchain.llms import OpenAI\\n\\nprompt = PromptTemplate.from_template(\"What is the {subject} of {object}?\")\\nllm = OpenAI(temperature=0)\\nchain = LLMChain(llm=llm, prompt=prompt)\\n\\nresponse = chain.run(subject=\"color\", object=\"sky\")\\nprint(response)\\n```\\n\\n### 2. **Prompt（提示）**\\n- Prompt 是 LLM 的输入，通常由模板生成。\\n- LangChain 提供了多种 Prompt 模板类型（如 `PromptTemplate`, `FewShotPromptTemplate` 等），方便构造复杂的输入。\\n\\n### 3. **Memory（记忆）**\\n- Memory 用于在多个 Chain 之间传递状态或上下文信息。\\n- 常见的 Memory 类型包括：\\n  - `SimpleMemory`\\n  - `ConversationBufferMemory`\\n  - `VectorStoreRetriever`\\n\\n### 4. **Agent（代理）**\\n- Agent 是一种高级功能，允许 LLM 自主选择工具或执行任务。\\n- 它可以结合多个 Chain 和工具，形成一个“智能助手”。\\n\\n#### 示例：\\n```python\\nfrom langchain.agents import initialize_agent, load_tools\\nfrom langchain.llms import OpenAI\\n\\nllm = OpenAI(temperature=0)\\ntools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\\nagent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)\\n\\nagent.run(\"What is the capital of France?\")\\n```\\n\\n### 5. **Vector Store（向量存储）**\\n- 用于存储和检索嵌入向量化的文本数据，常用于 RAG（Retrieval-Augmented Generation）系统中。\\n- 支持多种数据库（如 Pinecone、FAISS、Chroma、Weaviate 等）。\\n\\n---\\n\\n## 二、LangChain 的主要用途\\n\\n| 使用场景 | 说明 |\\n|----------|------|\\n| **RAG（检索增强生成）** | 结合向量数据库和 LLM，提高回答准确性 |\\n| **对话系统** | 构建智能客服、聊天机器人等 |\\n| **自动化流程** | 将 LLM 与外部工具（如数据库、API）结合，实现自动化任务 |\\n| **知识库问答** | 基于特定文档进行问答，适用于企业内部知识管理 |\\n\\n---\\n\\n## 三、LangChain 的优势\\n\\n| 特点 | 说明 |\\n|------|------|\\n| **模块化设计** | 易于扩展和定制 |\\n| **兼容性强** | 支持多种 LLM（如 GPT、LLaMA、ChatGLM 等） |\\n| **丰富的工具集** | 包括 Prompt、Memory、Agent、Vector Store 等 |\\n| **社区活跃** | 有大量开发者贡献插件和教程，生态完善 |\\n\\n---\\n\\n## 四、LangChain 的生态系统\\n\\nLangChain 已经发展出一套完整的生态系统，包括：\\n\\n- **LangChain Core**：基础库，包含 Chain、Prompt、Memory 等核心组件。\\n- **LangChain Hub**：一个共享的 Prompt 和 Agent 模板仓库。\\n- **LangChain Ecosystem**：包括各种插件、工具和第三方集成（如 `langchain-huggingface`、`langchain-chroma` 等）。\\n\\n---\\n\\n## 五、典型应用场景\\n\\n### 1. **智能客服**\\n- 使用 LLM + Vector Store 实现基于公司文档的自动问答。\\n- 使用 Agent 自动调用 API 或数据库获取信息。\\n\\n### 2. **数据分析助手**\\n- 用户输入自然语言问题，系统自动调用 SQL 查询并返回结果。\\n- 配合 LLM 进行解释和可视化建议。\\n\\n### 3. **内容生成**\\n- 根据用户指令自动生成文章、邮件、剧本等。\\n- 通过 Chain 组合多个 LLM 操作，提升生成质量。\\n\\n---\\n\\n## 六、总结\\n\\n> **LangChain 是一个强大的框架，旨在帮助开发者高效地构建基于大语言模型的应用程序。它提供了从简单提示模板到复杂代理系统的完整工具链，是连接 LLM 与现实世界应用的关键桥梁。**\\n\\n如果你需要具体的代码示例、最佳实践或如何与你使用的 LLM（如 Qwen、GPT、Bert 等）集成，我可以进一步为你详细讲解。' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 1119, 'prompt_tokens': 30, 'total_tokens': 1149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'qwen-turbo', 'system_fingerprint': None, 'id': 'chatcmpl-c274cf2e-9327-4104-8169-a6b72867e888', 'service_tier': None, 'finish_reason': 'stop', 'logprobs': None} id='run--02409ab2-4c32-45ef-b498-c856911a3755-0' usage_metadata={'input_tokens': 30, 'output_tokens': 1119, 'total_tokens': 1149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "3、 使用输出解析器",
   "id": "ff62ee0bd5e30c73"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T06:56:29.141024Z",
     "start_time": "2025-12-09T06:56:27.325651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser,JsonOutputParser\n",
    "\n",
    "# 初始化模型\n",
    "llm = ChatOpenAI(model=\"qwen-turbo\")\n",
    "\n",
    "# 创建提示模板\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"你是世界级的技术文档编写者。\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "# 使用输出解析器\n",
    "# output_parser = StrOutputParser()\n",
    "output_parser = JsonOutputParser()\n",
    "\n",
    "# 将其添加到上一个链中\n",
    "# chain = prompt | llm\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "# 调用它并提出同样的问题。答案是一个字符串，而不是ChatMessage\n",
    "# chain.invoke({\"input\": \"LangChain是什么?\"})\n",
    "chain.invoke({\"input\": \"LangChain是什么? 用JSON格式回复，问题用question，回答用answer\"})"
   ],
   "id": "a126f54136a311df",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'LangChain是什么?',\n",
       " 'answer': 'LangChain 是一个用于构建基于语言模型的应用程序的框架，它提供了一系列工具和库，帮助开发者更高效地集成和操作大型语言模型（LLM）。LangChain 支持多种语言模型，并提供了数据连接、链式处理、代理等功能，使开发者能够轻松构建复杂的自然语言处理应用。'}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "4、使用向量存储",
   "id": "cbf571ab083521b9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T07:00:43.141283Z",
     "start_time": "2025-12-09T07:00:42.051243Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 导入和使用 WebBaseLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "        web_path=\"https://www.gov.cn/zhengce/content/202512/content_7050164.htm\",\n",
    "        bs_kwargs=dict(parse_only=bs4.SoupStrainer(id=\"UCAP-CONTENT\"))\n",
    "    )\n",
    "docs = loader.load()\n",
    "# print(docs)\n",
    "\n",
    "# 对于嵌入模型，这里通过 API调用\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 使用分割器分割文档\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = text_splitter.split_documents(docs)\n",
    "print(len(documents))\n",
    "# 向量存储  embeddings 会将 documents 中的每个文本片段转换为向量，并将这些向量存储在 FAISS 向量数据库中\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ],
   "id": "bc54fd7e6e70929e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Error code: 404 - {'error': {'message': 'The model `text-embedding-ada-002` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}, 'request_id': '6bb90920-1420-4138-918f-25ef1be1b606'}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mNotFoundError\u001B[39m                             Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[28]\u001B[39m\u001B[32m, line 26\u001B[39m\n\u001B[32m     24\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(documents))\n\u001B[32m     25\u001B[39m \u001B[38;5;66;03m# 向量存储  embeddings 会将 documents 中的每个文本片段转换为向量，并将这些向量存储在 FAISS 向量数据库中\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m vector = \u001B[43mFAISS\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocuments\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\langchain_core\\vectorstores\\base.py:813\u001B[39m, in \u001B[36mVectorStore.from_documents\u001B[39m\u001B[34m(cls, documents, embedding, **kwargs)\u001B[39m\n\u001B[32m    810\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28many\u001B[39m(ids):\n\u001B[32m    811\u001B[39m         kwargs[\u001B[33m\"\u001B[39m\u001B[33mids\u001B[39m\u001B[33m\"\u001B[39m] = ids\n\u001B[32m--> \u001B[39m\u001B[32m813\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfrom_texts\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmetadatas\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001B[39m, in \u001B[36mfrom_texts\u001B[39m\u001B[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001B[39m\n\u001B[32m   1023\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001B[39;00m\n\u001B[32m   1024\u001B[39m \n\u001B[32m   1025\u001B[39m \u001B[33;03mThis is a user friendly interface that:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1039\u001B[39m \u001B[33;03m        faiss = FAISS.from_texts(texts, embeddings)\u001B[39;00m\n\u001B[32m   1040\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1041\u001B[39m embeddings = embedding.embed_documents(texts)\n\u001B[32m   1042\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m.__from(\n\u001B[32m-> \u001B[39m\u001B[32m1043\u001B[39m     texts,\n\u001B[32m   1044\u001B[39m     embeddings,\n\u001B[32m   1045\u001B[39m     embedding,\n\u001B[32m   1046\u001B[39m     metadatas=metadatas,\n\u001B[32m   1047\u001B[39m     ids=ids,\n\u001B[32m   1048\u001B[39m     **kwargs,\n\u001B[32m   1049\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:575\u001B[39m, in \u001B[36mOpenAIEmbeddings.embed_documents\u001B[39m\u001B[34m(self, texts, chunk_size)\u001B[39m\n\u001B[32m    572\u001B[39m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[32m    573\u001B[39m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[32m    574\u001B[39m engine = cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m.deployment)\n\u001B[32m--> \u001B[39m\u001B[32m575\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_len_safe_embeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    576\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[43m=\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunk_size\u001B[49m\n\u001B[32m    577\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:471\u001B[39m, in \u001B[36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[39m\u001B[34m(self, texts, engine, chunk_size)\u001B[39m\n\u001B[32m    469\u001B[39m batched_embeddings: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m]] = []\n\u001B[32m    470\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n\u001B[32m--> \u001B[39m\u001B[32m471\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    472\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m=\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43m_chunk_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_invocation_params\u001B[49m\n\u001B[32m    473\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    474\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[32m    475\u001B[39m         response = response.model_dump()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001B[39m, in \u001B[36mEmbeddings.create\u001B[39m\u001B[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    126\u001B[39m             embedding.embedding = np.frombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[32m    127\u001B[39m                 base64.b64decode(data), dtype=\u001B[33m\"\u001B[39m\u001B[33mfloat32\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    128\u001B[39m             ).tolist()\n\u001B[32m    130\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[32m--> \u001B[39m\u001B[32m132\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/embeddings\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mEmbeddingCreateParams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    135\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    137\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    138\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    139\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    140\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpost_parser\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    141\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    142\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mCreateEmbeddingResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    143\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1245\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1246\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1247\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1254\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1255\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1256\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1257\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1258\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1259\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1044\u001B[39m             err.response.read()\n\u001B[32m   1046\u001B[39m         log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1047\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1049\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m   1051\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mcould not resolve response (should never happen)\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mNotFoundError\u001B[39m: Error code: 404 - {'error': {'message': 'The model `text-embedding-ada-002` does not exist or you do not have access to it.', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}, 'request_id': '6bb90920-1420-4138-918f-25ef1be1b606'}"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T08:08:34.067964Z",
     "start_time": "2025-12-09T08:08:33.035784Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "input_text = \"衣服的质量杠杠的\"\n",
    "\n",
    "client = OpenAI(\n",
    "    # 若没有配置环境变量，请用阿里云百炼API Key将下行替换为：api_key=\"sk-xxx\",\n",
    "    # 新加坡和北京地域的API Key不同。获取API Key：https://help.aliyun.com/zh/model-studio/get-api-key\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),  \n",
    "    # 以下是北京地域base-url，如果使用新加坡地域的模型，需要将base_url替换为：https://dashscope-intl.aliyuncs.com/compatible-mode/v1\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"\n",
    ")\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "        web_path=\"https://www.gov.cn/zhengce/content/202512/content_7050164.htm\",\n",
    "        bs_kwargs=dict(parse_only=bs4.SoupStrainer(id=\"UCAP-CONTENT\"))\n",
    "    )\n",
    "docs = loader.load()\n",
    "# print(docs)\n",
    "document_texts = [doc.page_content for doc in docs]\n",
    "# print(document_texts)\n",
    "\n",
    "completion = client.embeddings.create(\n",
    "    model=\"text-embedding-v4\",\n",
    "    input=document_texts\n",
    ")\n",
    "\n",
    "# print(completion.model_dump_json())\n",
    "\n",
    "\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "documents = text_splitter.split_documents(document_texts)\n",
    "print(len(documents))\n",
    "# 向量存储  embeddings 会将 documents 中的每个文本片段转换为向量，并将这些向量存储在 FAISS 向量数据库中\n",
    "vector = FAISS.from_documents(documents, embeddings)\n"
   ],
   "id": "abc1d8fdde4125ca",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'page_content'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mAttributeError\u001B[39m                            Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[65]\u001B[39m\u001B[32m, line 38\u001B[39m\n\u001B[32m     35\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain_text_splitters\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m RecursiveCharacterTextSplitter\n\u001B[32m     37\u001B[39m text_splitter = RecursiveCharacterTextSplitter(chunk_size=\u001B[32m500\u001B[39m, chunk_overlap=\u001B[32m50\u001B[39m)\n\u001B[32m---> \u001B[39m\u001B[32m38\u001B[39m documents = \u001B[43mtext_splitter\u001B[49m\u001B[43m.\u001B[49m\u001B[43msplit_documents\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdocument_texts\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     39\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;28mlen\u001B[39m(documents))\n\u001B[32m     40\u001B[39m \u001B[38;5;66;03m# 向量存储  embeddings 会将 documents 中的每个文本片段转换为向量，并将这些向量存储在 FAISS 向量数据库中\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\langchain_text_splitters\\base.py:115\u001B[39m, in \u001B[36msplit_documents\u001B[39m\u001B[34m(self, documents)\u001B[39m\n\u001B[32m    113\u001B[39m current_doc: List[\u001B[38;5;28mstr\u001B[39m] = []\n\u001B[32m    114\u001B[39m total = \u001B[32m0\u001B[39m\n\u001B[32m--> \u001B[39m\u001B[32m115\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m splits:\n\u001B[32m    116\u001B[39m     _len = \u001B[38;5;28mself\u001B[39m._length_function(d)\n\u001B[32m    117\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[32m    118\u001B[39m         total + _len + (separator_len \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(current_doc) > \u001B[32m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[32m0\u001B[39m)\n\u001B[32m    119\u001B[39m         > \u001B[38;5;28mself\u001B[39m._chunk_size\n\u001B[32m    120\u001B[39m     ):\n",
      "\u001B[31mAttributeError\u001B[39m: 'str' object has no attribute 'page_content'"
     ]
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-09T08:11:52.227134Z",
     "start_time": "2025-12-09T08:11:50.389002Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "from langchain.schema import Document  # 引入 Document 类\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# 设置 API 密钥\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    base_url=\"https://dashscope.aliyuncs.com/compatible-mode/v1\"  # 阿里云百炼的 base URL\n",
    ")\n",
    "\n",
    "# 加载网页内容\n",
    "loader = WebBaseLoader(\n",
    "    web_path=\"https://www.gov.cn/zhengce/content/202512/content_7050164.htm\",\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(id=\"UCAP-CONTENT\"))\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# 提取文本内容\n",
    "document_texts = [doc.page_content for doc in docs]\n",
    "\n",
    "# 创建 Document 对象列表，确保每个文档包含 'page_content'\n",
    "documents = [Document(page_content=text) for text in document_texts]\n",
    "\n",
    "# 生成嵌入（embeddings）\n",
    "completion = client.embeddings.create(\n",
    "    model=\"text-embedding-v4\",  # 使用百炼嵌入模型\n",
    "    input=document_texts  # 输入文本列表\n",
    ")\n",
    "\n",
    "# 获取嵌入向量\n",
    "embeddings = [embedding.embedding for embedding in completion.data]\n",
    "\n",
    "# 分割文档\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "split_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "# 确保我们传递的是纯文本字符串\n",
    "split_texts = [doc.page_content for doc in split_documents]\n",
    "\n",
    "# 使用阿里云百炼支持的嵌入模型\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-v4\")  # 替换为你可用的有效模型\n",
    "print(embeddings)\n",
    "\n",
    "# 将文本嵌入存储到 FAISS 向量数据库\n",
    "vector_store = FAISS.from_texts(split_texts, embeddings)\n"
   ],
   "id": "814db6f72d666c0f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.embeddings.Embeddings object at 0x000001DF0F23C950> async_client=<openai.resources.embeddings.AsyncEmbeddings object at 0x000001DF0F0D8860> model='text-embedding-v4' dimensions=None deployment='text-embedding-ada-002' openai_api_version=None openai_api_base=None openai_api_type=None openai_proxy=None embedding_ctx_length=8191 openai_api_key=SecretStr('**********') openai_organization=None allowed_special=None disallowed_special=None chunk_size=1000 max_retries=2 request_timeout=None headers=None tiktoken_enabled=True tiktoken_model_name=None show_progress_bar=False model_kwargs={} skip_empty=False default_headers=None default_query=None retry_min_seconds=4 retry_max_seconds=20 http_client=None http_async_client=None check_embedding_ctx_length=True\n"
     ]
    },
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': '<400> InternalError.Algo.InvalidParameter: Value error, contents is neither str nor list of str.: input.contents', 'type': 'InvalidParameter', 'param': None, 'code': 'InvalidParameter'}, 'id': 'dbf6a226-55f8-4419-b892-dc883788c0c4', 'request_id': 'dbf6a226-55f8-4419-b892-dc883788c0c4'}",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mBadRequestError\u001B[39m                           Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[68]\u001B[39m\u001B[32m, line 50\u001B[39m\n\u001B[32m     47\u001B[39m \u001B[38;5;28mprint\u001B[39m(embeddings)\n\u001B[32m     49\u001B[39m \u001B[38;5;66;03m# 将文本嵌入存储到 FAISS 向量数据库\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m50\u001B[39m vector_store = \u001B[43mFAISS\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_texts\u001B[49m\u001B[43m(\u001B[49m\u001B[43msplit_texts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membeddings\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\langchain_community\\vectorstores\\faiss.py:1043\u001B[39m, in \u001B[36mfrom_texts\u001B[39m\u001B[34m(cls, texts, embedding, metadatas, ids, **kwargs)\u001B[39m\n\u001B[32m   1023\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Construct FAISS wrapper from raw documents.\u001B[39;00m\n\u001B[32m   1024\u001B[39m \n\u001B[32m   1025\u001B[39m \u001B[33;03mThis is a user friendly interface that:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1039\u001B[39m \u001B[33;03m        faiss = FAISS.from_texts(texts, embeddings)\u001B[39;00m\n\u001B[32m   1040\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1041\u001B[39m embeddings = embedding.embed_documents(texts)\n\u001B[32m   1042\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m.__from(\n\u001B[32m-> \u001B[39m\u001B[32m1043\u001B[39m     texts,\n\u001B[32m   1044\u001B[39m     embeddings,\n\u001B[32m   1045\u001B[39m     embedding,\n\u001B[32m   1046\u001B[39m     metadatas=metadatas,\n\u001B[32m   1047\u001B[39m     ids=ids,\n\u001B[32m   1048\u001B[39m     **kwargs,\n\u001B[32m   1049\u001B[39m )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:575\u001B[39m, in \u001B[36mOpenAIEmbeddings.embed_documents\u001B[39m\u001B[34m(self, texts, chunk_size)\u001B[39m\n\u001B[32m    572\u001B[39m \u001B[38;5;66;03m# NOTE: to keep things simple, we assume the list may contain texts longer\u001B[39;00m\n\u001B[32m    573\u001B[39m \u001B[38;5;66;03m#       than the maximum context and use length-safe embedding function.\u001B[39;00m\n\u001B[32m    574\u001B[39m engine = cast(\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m.deployment)\n\u001B[32m--> \u001B[39m\u001B[32m575\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_get_len_safe_embeddings\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    576\u001B[39m \u001B[43m    \u001B[49m\u001B[43mtexts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mengine\u001B[49m\u001B[43m=\u001B[49m\u001B[43mengine\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mchunk_size\u001B[49m\u001B[43m=\u001B[49m\u001B[43mchunk_size\u001B[49m\n\u001B[32m    577\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\langchain_openai\\embeddings\\base.py:471\u001B[39m, in \u001B[36mOpenAIEmbeddings._get_len_safe_embeddings\u001B[39m\u001B[34m(self, texts, engine, chunk_size)\u001B[39m\n\u001B[32m    469\u001B[39m batched_embeddings: \u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mlist\u001B[39m[\u001B[38;5;28mfloat\u001B[39m]] = []\n\u001B[32m    470\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m _iter:\n\u001B[32m--> \u001B[39m\u001B[32m471\u001B[39m     response = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mclient\u001B[49m\u001B[43m.\u001B[49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    472\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m=\u001B[49m\u001B[43mtokens\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mi\u001B[49m\u001B[43m \u001B[49m\u001B[43m+\u001B[49m\u001B[43m \u001B[49m\u001B[43m_chunk_size\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_invocation_params\u001B[49m\n\u001B[32m    473\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    474\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, \u001B[38;5;28mdict\u001B[39m):\n\u001B[32m    475\u001B[39m         response = response.model_dump()\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\openai\\resources\\embeddings.py:132\u001B[39m, in \u001B[36mEmbeddings.create\u001B[39m\u001B[34m(self, input, model, dimensions, encoding_format, user, extra_headers, extra_query, extra_body, timeout)\u001B[39m\n\u001B[32m    126\u001B[39m             embedding.embedding = np.frombuffer(  \u001B[38;5;66;03m# type: ignore[no-untyped-call]\u001B[39;00m\n\u001B[32m    127\u001B[39m                 base64.b64decode(data), dtype=\u001B[33m\"\u001B[39m\u001B[33mfloat32\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m    128\u001B[39m             ).tolist()\n\u001B[32m    130\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\n\u001B[32m--> \u001B[39m\u001B[32m132\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    133\u001B[39m \u001B[43m    \u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43m/embeddings\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m    134\u001B[39m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparams\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43membedding_create_params\u001B[49m\u001B[43m.\u001B[49m\u001B[43mEmbeddingCreateParams\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    135\u001B[39m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[43m=\u001B[49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m    136\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    137\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    138\u001B[39m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m=\u001B[49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    139\u001B[39m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m=\u001B[49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    140\u001B[39m \u001B[43m        \u001B[49m\u001B[43mpost_parser\u001B[49m\u001B[43m=\u001B[49m\u001B[43mparser\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    141\u001B[39m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    142\u001B[39m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m=\u001B[49m\u001B[43mCreateEmbeddingResponse\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m    143\u001B[39m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1259\u001B[39m, in \u001B[36mSyncAPIClient.post\u001B[39m\u001B[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[39m\n\u001B[32m   1245\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mpost\u001B[39m(\n\u001B[32m   1246\u001B[39m     \u001B[38;5;28mself\u001B[39m,\n\u001B[32m   1247\u001B[39m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m   1254\u001B[39m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] | \u001B[38;5;28;01mNone\u001B[39;00m = \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[32m   1255\u001B[39m ) -> ResponseT | _StreamT:\n\u001B[32m   1256\u001B[39m     opts = FinalRequestOptions.construct(\n\u001B[32m   1257\u001B[39m         method=\u001B[33m\"\u001B[39m\u001B[33mpost\u001B[39m\u001B[33m\"\u001B[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001B[32m   1258\u001B[39m     )\n\u001B[32m-> \u001B[39m\u001B[32m1259\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m=\u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\openai\\_base_client.py:1047\u001B[39m, in \u001B[36mSyncAPIClient.request\u001B[39m\u001B[34m(self, cast_to, options, stream, stream_cls)\u001B[39m\n\u001B[32m   1044\u001B[39m             err.response.read()\n\u001B[32m   1046\u001B[39m         log.debug(\u001B[33m\"\u001B[39m\u001B[33mRe-raising status error\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1047\u001B[39m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m._make_status_error_from_response(err.response) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1049\u001B[39m     \u001B[38;5;28;01mbreak\u001B[39;00m\n\u001B[32m   1051\u001B[39m \u001B[38;5;28;01massert\u001B[39;00m response \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[33m\"\u001B[39m\u001B[33mcould not resolve response (should never happen)\u001B[39m\u001B[33m\"\u001B[39m\n",
      "\u001B[31mBadRequestError\u001B[39m: Error code: 400 - {'error': {'message': '<400> InternalError.Algo.InvalidParameter: Value error, contents is neither str nor list of str.: input.contents', 'type': 'InvalidParameter', 'param': None, 'code': 'InvalidParameter'}, 'id': 'dbf6a226-55f8-4419-b892-dc883788c0c4', 'request_id': 'dbf6a226-55f8-4419-b892-dc883788c0c4'}"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T02:38:51.870631Z",
     "start_time": "2025-12-11T02:38:51.495972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.example_selectors import SemanticSimilarityExampleSelector\n",
    "import os\n",
    "import dotenv\n",
    "\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "# 创建阿里云的嵌入模型实例\n",
    "embeddings_model = DashScopeEmbeddings(\n",
    "    model=\"text-embedding-v4\",  # 选择适当的模型，例如 text-embedding-v4\n",
    "    dashscope_api_key=os.getenv(\"DASHSCOPE_API_KEY\"),\n",
    "    # base_url=os.getenv(\"DASHSCOPE_API_KEY\")\n",
    ")\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "# 加载网页内容\n",
    "loader = WebBaseLoader(\n",
    "    web_path=\"https://www.gov.cn/zhengce/content/202512/content_7050164.htm\",\n",
    "    bs_kwargs=dict(parse_only=bs4.SoupStrainer(id=\"UCAP-CONTENT\"))\n",
    ")\n",
    "docs = loader.load()\n",
    "# print(docs)\n",
    "\n",
    "# 提取文本内容\n",
    "document_texts = [doc.page_content for doc in docs]\n",
    "# print(document_texts)\n",
    "\n",
    "# example_selector = SemanticSimilarityExampleSelector.from_examples(\n",
    "#     docs,\n",
    "#     embeddings_model,  # 使用阿里云的嵌入模型\n",
    "#     Chroma,\n",
    "#     k=1,\n",
    "# )\n",
    "\n",
    "\n",
    "question = \"这个网址内容说了什么?\"\n",
    "selected_examples = example_selector.select_examples({\"question\": question})\n",
    "print(f\"与输入最相似的示例：{selected_examples}\")\n"
   ],
   "id": "866563da33773ceb",
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Document' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mTypeError\u001B[39m                                 Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[6]\u001B[39m\u001B[32m, line 31\u001B[39m\n\u001B[32m     28\u001B[39m document_texts = [doc.page_content \u001B[38;5;28;01mfor\u001B[39;00m doc \u001B[38;5;129;01min\u001B[39;00m docs]\n\u001B[32m     29\u001B[39m \u001B[38;5;66;03m# print(document_texts)\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m31\u001B[39m example_selector = \u001B[43mSemanticSimilarityExampleSelector\u001B[49m\u001B[43m.\u001B[49m\u001B[43mfrom_examples\u001B[49m\u001B[43m(\u001B[49m\n\u001B[32m     32\u001B[39m \u001B[43m    \u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     33\u001B[39m \u001B[43m    \u001B[49m\u001B[43membeddings_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# 使用阿里云的嵌入模型\u001B[39;49;00m\n\u001B[32m     34\u001B[39m \u001B[43m    \u001B[49m\u001B[43mChroma\u001B[49m\u001B[43m,\u001B[49m\n\u001B[32m     35\u001B[39m \u001B[43m    \u001B[49m\u001B[43mk\u001B[49m\u001B[43m=\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[32m     36\u001B[39m \u001B[43m)\u001B[49m\n\u001B[32m     39\u001B[39m question = \u001B[33m\"\u001B[39m\u001B[33m这个网址内容说了什么?\u001B[39m\u001B[33m\"\u001B[39m\n\u001B[32m     40\u001B[39m selected_examples = example_selector.select_examples({\u001B[33m\"\u001B[39m\u001B[33mquestion\u001B[39m\u001B[33m\"\u001B[39m: question})\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\langchain_core\\example_selectors\\semantic_similarity.py:170\u001B[39m, in \u001B[36mSemanticSimilarityExampleSelector.from_examples\u001B[39m\u001B[34m(cls, examples, embeddings, vectorstore_cls, k, input_keys, example_keys, vectorstore_kwargs, **vectorstore_cls_kwargs)\u001B[39m\n\u001B[32m    138\u001B[39m \u001B[38;5;129m@classmethod\u001B[39m\n\u001B[32m    139\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mfrom_examples\u001B[39m(\n\u001B[32m    140\u001B[39m     \u001B[38;5;28mcls\u001B[39m,\n\u001B[32m   (...)\u001B[39m\u001B[32m    149\u001B[39m     **vectorstore_cls_kwargs: Any,\n\u001B[32m    150\u001B[39m ) -> SemanticSimilarityExampleSelector:\n\u001B[32m    151\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Create k-shot example selector using example list and embeddings.\u001B[39;00m\n\u001B[32m    152\u001B[39m \n\u001B[32m    153\u001B[39m \u001B[33;03m    Reshuffles examples dynamically based on query similarity.\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m    168\u001B[39m \u001B[33;03m        The ExampleSelector instantiated, backed by a vector store.\u001B[39;00m\n\u001B[32m    169\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m170\u001B[39m     string_examples = [\u001B[38;5;28;43mcls\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_example_to_text\u001B[49m\u001B[43m(\u001B[49m\u001B[43meg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_keys\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m eg \u001B[38;5;129;01min\u001B[39;00m examples]\n\u001B[32m    171\u001B[39m     vectorstore = vectorstore_cls.from_texts(\n\u001B[32m    172\u001B[39m         string_examples, embeddings, metadatas=examples, **vectorstore_cls_kwargs\n\u001B[32m    173\u001B[39m     )\n\u001B[32m    174\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\n\u001B[32m    175\u001B[39m         vectorstore=vectorstore,\n\u001B[32m    176\u001B[39m         k=k,\n\u001B[32m   (...)\u001B[39m\u001B[32m    179\u001B[39m         vectorstore_kwargs=vectorstore_kwargs,\n\u001B[32m    180\u001B[39m     )\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\langchain_core\\example_selectors\\semantic_similarity.py:57\u001B[39m, in \u001B[36m_VectorStoreExampleSelector._example_to_text\u001B[39m\u001B[34m(example, input_keys)\u001B[39m\n\u001B[32m     55\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m input_keys:\n\u001B[32m     56\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m.join(sorted_values({key: example[key] \u001B[38;5;28;01mfor\u001B[39;00m key \u001B[38;5;129;01min\u001B[39;00m input_keys}))\n\u001B[32m---> \u001B[39m\u001B[32m57\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[33m\"\u001B[39m\u001B[33m \u001B[39m\u001B[33m\"\u001B[39m.join(\u001B[43msorted_values\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexample\u001B[49m\u001B[43m)\u001B[49m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32mD:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\langchain_core\\example_selectors\\semantic_similarity.py:28\u001B[39m, in \u001B[36msorted_values\u001B[39m\u001B[34m(values)\u001B[39m\n\u001B[32m     18\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34msorted_values\u001B[39m(values: \u001B[38;5;28mdict\u001B[39m[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]) -> \u001B[38;5;28mlist\u001B[39m[Any]:\n\u001B[32m     19\u001B[39m \u001B[38;5;250m    \u001B[39m\u001B[33;03m\"\"\"Return a list of values in dict sorted by key.\u001B[39;00m\n\u001B[32m     20\u001B[39m \n\u001B[32m     21\u001B[39m \u001B[33;03m    Args:\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m     26\u001B[39m \u001B[33;03m        A list of values in dict sorted by key.\u001B[39;00m\n\u001B[32m     27\u001B[39m \u001B[33;03m    \"\"\"\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m28\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mvalues\u001B[49m\u001B[43m[\u001B[49m\u001B[43mval\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m val \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28msorted\u001B[39m(values)]\n",
      "\u001B[31mTypeError\u001B[39m: 'Document' object is not subscriptable"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "5 RAG(检索增强生成)",
   "id": "78fcc323c8fcfaa1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retriever.search_kwargs = {\"k\": 3}\n",
    "docs = retriever.invoke(\"建设用地使用权是什么？\")\n",
    "\n",
    "# for i,doc in enumerate(docs):\n",
    "#     print(f\"⭐第{i+1}条规定：\")\n",
    "#     print(doc)\n",
    "\n",
    "# 6.定义提示词模版\n",
    "prompt_template = \"\"\"\n",
    "你是一个问答机器人。\n",
    "你的任务是根据下述给定的已知信息回答用户问题。\n",
    "确保你的回复完全依据下述已知信息。不要编造答案。\n",
    "如果下述已知信息不足以回答用户的问题，请直接回复\"我无法回答您的问题\"。\n",
    "\n",
    "已知信息:\n",
    "{info}\n",
    "\n",
    "用户问：\n",
    "{question}\n",
    "\n",
    "请用中文回答用户问题。\n",
    "\"\"\"\n",
    "# 7.得到提示词模版对象\n",
    "template = PromptTemplate.from_template(prompt_template)\n",
    "\n",
    "# 8.得到提示词对象\n",
    "prompt = template.format(info=docs, question='最近发生的新闻是什么？')\n",
    "\n",
    "## 9. 调用LLM\n",
    "response = llm.invoke(prompt)\n",
    "print(response.content)"
   ],
   "id": "311a5e7dcce1c411"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "6、使用Agent",
   "id": "b512de2bc49f2851"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "# 检索器工具\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"CivilCodeRetriever\",\n",
    "    \"搜索有关中华人民共和国民法典的信息。关于中华人民共和国民法典的任何问题，您必须使用此工具!\",\n",
    ")\n",
    "\n",
    "tools = [retriever_tool]\n",
    "\n",
    "from langchain import hub\n",
    "from langchain.agents import create_openai_functions_agent\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "# https://smith.langchain.com/hub\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "\n",
    "agent = create_openai_functions_agent(llm, tools, prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)\n",
    "\n",
    "# 运行代理\n",
    "agent_executor.invoke({\"input\": \"最近发生的新闻是什么？\"})"
   ],
   "id": "89b45bd3aa9a53a8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "OLLAMA-Test",
   "id": "66f98e8a831326c2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:10:27.434276Z",
     "start_time": "2025-12-11T03:10:27.350802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.embeddings import BaseEmbedding\n",
    "import requests\n",
    "\n",
    "class OllamaEmbedding(BaseEmbedding):\n",
    "    def __init__(self, model_name=\"llama\", api_url=\"http://localhost:11434/v1/complete\"):\n",
    "        self.model_name = model_name\n",
    "        self.api_url = api_url\n",
    "\n",
    "    def embed_query(self, query: str):\n",
    "        \"\"\"\n",
    "        调用 Ollama API 生成查询的嵌入。\n",
    "        \"\"\"\n",
    "        data = {\n",
    "            \"model\": self.model_name,\n",
    "            \"prompt\": query,\n",
    "            \"max_tokens\": 100,  # 可根据需要调整\n",
    "        }\n",
    "        response = requests.post(self.api_url, json=data)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()['choices'][0]['text']\n",
    "        else:\n",
    "            raise Exception(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "    def embed_documents(self, documents: list):\n",
    "        \"\"\"\n",
    "        批量调用 Ollama API 生成文档的嵌入。\n",
    "        \"\"\"\n",
    "        return [self.embed_query(doc) for doc in documents]\n"
   ],
   "id": "b773eb8bb6ba63dd",
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'BaseEmbedding' from 'langchain.embeddings' (D:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\langchain\\embeddings\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mImportError\u001B[39m                               Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 1\u001B[39m\n\u001B[32m----> \u001B[39m\u001B[32m1\u001B[39m \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mlangchain\u001B[39;00m\u001B[34;01m.\u001B[39;00m\u001B[34;01membeddings\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;28;01mimport\u001B[39;00m BaseEmbedding\n\u001B[32m      2\u001B[39m \u001B[38;5;28;01mimport\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mrequests\u001B[39;00m\n\u001B[32m      4\u001B[39m \u001B[38;5;28;01mclass\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mOllamaEmbedding\u001B[39;00m(BaseEmbedding):\n",
      "\u001B[31mImportError\u001B[39m: cannot import name 'BaseEmbedding' from 'langchain.embeddings' (D:\\zy\\software\\Miniconda3\\install\\envs\\myenv\\Lib\\site-packages\\langchain\\embeddings\\__init__.py)"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-11T03:08:35.711049Z",
     "start_time": "2025-12-11T03:08:35.706425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.agents import initialize_agent, Tool, AgentType\n",
    "\n",
    "# 创建 Ollama 嵌入模型实例\n",
    "ollama_embedding = OllamaEmbedding(model_name=\"llama\", api_url=\"http://localhost:11434/v1/complete\")\n",
    "\n",
    "# 示例文档\n",
    "documents = [\n",
    "    \"Ollama is a framework for large language models.\",\n",
    "    \"LangChain integrates with different LLMs for various NLP tasks.\",\n",
    "    \"FAISS is a library for efficient similarity search and clustering of dense vectors.\"\n",
    "]\n",
    "\n",
    "# 使用 Ollama 嵌入模型生成文档的嵌入\n",
    "document_embeddings = ollama_embedding.embed_documents(documents)\n",
    "\n",
    "# 创建 FAISS 向量存储\n",
    "faiss_store = FAISS.from_texts(documents, ollama_embedding)\n",
    "\n",
    "# 创建查询模板\n",
    "query_prompt = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=\"Find the most relevant document for the following query: {query}\"\n",
    ")\n",
    "\n",
    "# 设置用于检索的检索链\n",
    "retriever = faiss_store.as_retriever()\n",
    "\n",
    "# 创建问答链\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=ollama_embedding,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "# 测试查询\n",
    "query = \"What is Ollama?\"\n",
    "response = qa_chain.run(query)\n",
    "\n",
    "print(response)\n"
   ],
   "id": "c9c3febff5ad3e5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 404 404 page not found\n"
     ]
    }
   ],
   "execution_count": 13
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
